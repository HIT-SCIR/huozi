<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->

<div align="center">
<h1>
  <img src="image/huozi-logo.jpg" width="30" /> æ´»å­—é€šç”¨å¤§æ¨¡å‹
</h1>
</div>
</p>

<div align="center">
<a href="https://github.com/HIT-SCIR/huozi/pulls">
<image src="https://img.shields.io/badge/PRs-welcome-brightgreen">
</a>
<a href="https://github.com/HIT-SCIR/huozi/pulls">
<image src="https://img.shields.io/badge/License-Apache_2.0-green.svg">
</a>
<!-- <h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/HIT-SCIR/huozi/blob/main/README_EN.md">English</a>
    <p>
</h4> -->
</div>

## ğŸ‰ æ›´æ–°

- [2024-02-09] å‘å¸ƒæ´»å­—3.0ç‰ˆæœ¬å’Œä¸­æ–‡MT-Benchæ•°æ®é›†
- [2023-08-06] å‘å¸ƒæ´»å­—1.0å’Œæ´»å­—2.0ç‰ˆæœ¬
- [2023-05-04] å‘å¸ƒã€ŠChatGPTè°ƒç ”æŠ¥å‘Šã€‹ï¼›å†…æµ‹æ´»å­—å¤§æ¨¡å‹

## ğŸ”– ç›®å½•

|ç« èŠ‚|è¯´æ˜|
|---|---|
|[ğŸ’ğŸ»â€â™‚ å¼€æºæ¸…å•](#-å¼€æºæ¸…å•)|æœ¬ä»“åº“å¼€æºé¡¹ç›®æ¸…å•|
|[ğŸ’¡ æ¨¡å‹ä»‹ç»](#-æ¨¡å‹ä»‹ç»)|ç®€è¦ä»‹ç»æ´»å­—æ¨¡å‹ç»“æ„å’Œè®­ç»ƒè¿‡ç¨‹|
|[ğŸ“¥ æ¨¡å‹ä¸‹è½½](#-æ¨¡å‹ä¸‹è½½)|æ´»å­—æ¨¡å‹ä¸‹è½½é“¾æ¥|
|[ğŸ’» æ¨¡å‹æ¨ç†](#-æ¨¡å‹æ¨ç†)|æ´»å­—æ¨¡å‹æ¨ç†æ ·ä¾‹ï¼ŒåŒ…æ‹¬vLLMæ¨ç†åŠ é€Ÿã€llama.cppé‡åŒ–æ¨ç†ç­‰æ¡†æ¶çš„ä½¿ç”¨æµç¨‹|
|[ğŸ“ˆ æ¨¡å‹æ€§èƒ½](#-æ¨¡å‹æ€§èƒ½)|æ´»å­—æ¨¡å‹åœ¨ä¸»æµè¯„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½|
|[ğŸ—‚ ç”Ÿæˆæ ·ä¾‹](#-ç”Ÿæˆæ ·ä¾‹)|æ´»å­—æ¨¡å‹å®é™…ç”Ÿæˆæ•ˆæœæ ·ä¾‹|

## ğŸ’ğŸ»â€â™‚ å¼€æºæ¸…å•
![](image/models-v3.png)
- **æ´»å­— 3.0**: [[æ¨¡å‹æƒé‡](#-æ¨¡å‹ä¸‹è½½)]
    - æ´»å­—3.0ä¸ºä¸€ä¸ªç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ï¼Œæ”¯æŒ32Kä¸Šä¸‹æ–‡ï¼Œå…·æœ‰ä¸°å¯Œçš„ä¸­ã€è‹±æ–‡çŸ¥è¯†å’Œå¼ºå¤§çš„æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚æ´»å­—3.0è¾ƒæ—§ç‰ˆæ´»å­—å…·æœ‰æ›´å¼ºçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚
- **ä¸­æ–‡MT-Bench**: [[æ•°æ®é›†](data/mt-bench-zh/)]
    - æœ¬æ•°æ®é›†æ˜¯è‹±æ–‡MT-Benchå¯¹è¯èƒ½åŠ›è¯„æµ‹æ•°æ®é›†çš„ä¸­æ–‡ç‰ˆã€‚å®ƒåŒ…å«äº†ä¸€ç³»åˆ—å¤šè½®å¯¹è¯é—®é¢˜ï¼Œæ¯ä¸€ç»„é—®é¢˜éƒ½ç»è¿‡äº†ç²¾å¿ƒçš„äººå·¥æ ¡å¯¹ï¼Œå¹¶ä¸ºé€‚åº”ä¸­æ–‡è¯­å¢ƒè¿›è¡Œäº†å¿…è¦çš„è°ƒæ•´ã€‚
- **ã€ŠChatGPT è°ƒç ”æŠ¥å‘Šã€‹**: [[PDF](https://github.com/HIT-SCIR/huozi/blob/main/pdf/chatgpt_book.pdf)]
    - å“ˆå·¥å¤§è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶æ‰€ç»„ç»‡å¤šä½è€å¸ˆå’ŒåŒå­¦æ’°å†™äº†æœ¬è°ƒç ”æŠ¥å‘Šï¼Œä»æŠ€æœ¯åŸç†ã€åº”ç”¨åœºæ™¯ã€æœªæ¥å‘å±•ç­‰æ–¹é¢å¯¹ChatGPTè¿›è¡Œäº†å°½é‡è¯¦å°½çš„ä»‹ç»åŠæ€»ç»“ã€‚
- **æ´»å­— 2.0**: [[æ¨¡å‹æƒé‡](https://huggingface.co/HIT-SCIR/huozi-7b-rlhf)] [[RLHFæ•°æ®](data/huozi-rlhf/huozi_rlhf_data.csv)]
    - åœ¨æ´»å­—1.0åŸºç¡€ä¸Šï¼Œé€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¨¡å‹å›å¤è´¨é‡ï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆäººç±»åå¥½ã€‚ç›¸è¾ƒäºä¸Šä¸€ä¸ªç‰ˆæœ¬å¹³å‡é•¿åº¦æ˜æ˜¾æé«˜ï¼Œéµä»æŒ‡ä»¤çš„èƒ½åŠ›æ›´å¼ºï¼Œé€»è¾‘æ›´åŠ æ¸…æ™°ã€‚
    - 16.9k äººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®ï¼Œå›å¤æ¥è‡ªæ´»å­—æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚
- **æ´»å­— 1.0**: [[æ¨¡å‹æƒé‡](https://huggingface.co/HIT-SCIR/huozi-7b-sft)]
    - åœ¨Bloomæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œåœ¨å¤§çº¦ 150 äº¿ tokens ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ï¼Œå…·æœ‰æ›´å¼ºçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€æ›´å¥½çš„å®‰å…¨æ€§ã€‚

## ğŸ’¡ æ¨¡å‹ä»‹ç»

å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œå¹¶åœ¨å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­å±•ç°äº†å…¶å¼ºå¤§çš„æ½œåŠ›ã€‚è¿™ä¸€æŠ€æœ¯ä¸ä»…å¸å¼•äº†å­¦æœ¯ç•Œçš„å¹¿æ³›å…³æ³¨ï¼Œä¹Ÿæˆä¸ºäº†å·¥ä¸šç•Œçš„çƒ­ç‚¹ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ç¤¾ä¼šè®¡ç®—ä¸ä¿¡æ¯æ£€ç´¢ç ”ç©¶ä¸­å¿ƒï¼ˆHIT-SCIRï¼‰è¿‘æœŸæ¨å‡ºäº†æœ€æ–°æˆæœâ€”â€”**æ´»å­—3.0**ï¼Œè‡´åŠ›äºä¸ºè‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å’Œå®é™…åº”ç”¨æä¾›æ›´å¤šå¯èƒ½æ€§å’Œé€‰æ‹©ã€‚

æ´»å­—3.0æ˜¯åŸºäºChinese-Mixtral-8x7Bï¼Œåœ¨å¤§çº¦30ä¸‡è¡ŒæŒ‡ä»¤æ•°æ®ä¸Šå¾®è°ƒå¾—åˆ°çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ”¯æŒ**32Kä¸Šä¸‹æ–‡**ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬ã€‚æ´»å­—3.0ç»§æ‰¿äº†åŸºåº§æ¨¡å‹ä¸°å¯Œçš„**ä¸­è‹±æ–‡çŸ¥è¯†**ï¼Œå¹¶åœ¨**æ•°å­¦æ¨ç†**ã€**ä»£ç ç”Ÿæˆ**ç­‰ä»»åŠ¡ä¸Šå…·æœ‰å¼ºå¤§æ€§èƒ½ã€‚ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œæ´»å­—3.0è¿˜åœ¨**æŒ‡ä»¤éµå¾ªèƒ½åŠ›**å’Œ**å®‰å…¨æ€§**æ–¹é¢å®ç°äº†æ˜¾è‘—æå‡ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€æºäº†**ä¸­æ–‡MT-Benchæ•°æ®é›†**ã€‚è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡å¼€æ”¾é—®é¢˜é›†ï¼ŒåŒ…æ‹¬80ç»„å¯¹è¯ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„å¤šè½®å¯¹è¯å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚è¯¥æ•°æ®é›†æ˜¯æ ¹æ®åŸå§‹MT-Benchç¿»è¯‘å¾—æ¥çš„ï¼Œæ¯ç»„é—®é¢˜å‡ç»è¿‡äººå·¥æ ¡å¯¹å’Œä¸­æ–‡è¯­å¢ƒä¸‹çš„é€‚å½“è°ƒæ•´ã€‚æˆ‘ä»¬è¿˜å¯¹åŸå§‹MT-Benchä¸­çš„éƒ¨åˆ†é”™è¯¯ç­”æ¡ˆè¿›è¡Œäº†ä¿®æ­£ã€‚

> [!IMPORTANT]
> æ´»å­—ç³»åˆ—æ¨¡å‹ä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨ç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†ç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚

> æ´»å­—1.0å’Œæ´»å­—2.0çš„æ–‡æ¡£è¯·è§[æ­¤å¤„](README-v1v2.md)ã€‚

### æ¨¡å‹ç»“æ„

æ´»å­—3.0æ˜¯ä¸€ä¸ªç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆSMoEï¼‰ï¼Œä½¿ç”¨äº†Mixtral-8x7Bçš„æ¨¡å‹ç»“æ„ã€‚å®ƒåŒºåˆ«äºLLaMAã€BLOOMç­‰å¸¸è§æ¨¡å‹ï¼Œæ´»å­—3.0çš„æ¯ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFFNï¼‰å±‚è¢«æ›¿æ¢ä¸ºäº†â€œä¸“å®¶å±‚â€ï¼Œè¯¥å±‚åŒ…å«8ä¸ªFFNå’Œä¸€ä¸ªâ€œè·¯ç”±å™¨â€ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ç‹¬ç«‹åœ°å°†æ¯ä¸ªTokenè·¯ç”±åˆ°æœ€é€‚åˆå¤„ç†å®ƒçš„ä¸¤ä¸ªä¸“å®¶ä¸­ã€‚æ´»å­—3.0å…±æ‹¥æœ‰46.7Bä¸ªå‚æ•°ï¼Œä½†å¾—ç›Šäºå…¶ç¨€ç–æ¿€æ´»çš„ç‰¹æ€§ï¼Œå®é™…æ¨ç†æ—¶ä»…éœ€æ¿€æ´»13Bå‚æ•°ï¼Œæœ‰æ•ˆæå‡äº†è®¡ç®—æ•ˆç‡å’Œå¤„ç†é€Ÿåº¦ã€‚

![](image/smoe.png)

### è®­ç»ƒè¿‡ç¨‹

ç”±äºMixtral-8x7Bè¯è¡¨ä¸æ”¯æŒä¸­æ–‡ï¼Œå› æ­¤å¯¹ä¸­æ–‡çš„ç¼–è§£ç æ•ˆç‡è¾ƒä½ï¼Œé™åˆ¶äº†ä¸­æ–‡åœºæ™¯ä¸‹çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬é¦–å…ˆåŸºäºMixtral-8x7Bè¿›è¡Œäº†ä¸­æ–‡æ‰©è¯è¡¨å¢é‡é¢„è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ä¸­æ–‡çš„ç¼–è§£ç æ•ˆç‡ï¼Œå¹¶ä½¿æ¨¡å‹å…·å¤‡äº†å¼ºå¤§çš„ä¸­æ–‡ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚è¿™é¡¹æˆæœåä¸º[Chinese-Mixtral-8x7B](https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B)ï¼Œæˆ‘ä»¬å·²äº2024å¹´1æœˆ18æ—¥å¼€æºäº†å…¶æ¨¡å‹æƒé‡å’Œè®­ç»ƒä»£ç ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œæœ€ç»ˆæ¨å‡ºäº†æ´»å­—3.0ã€‚è¿™ä¸€ç‰ˆæœ¬çš„ä¸­æ–‡ç¼–ç ã€æŒ‡ä»¤éµå¾ªã€å®‰å…¨å›å¤ç­‰èƒ½åŠ›éƒ½æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“¥ æ¨¡å‹ä¸‹è½½

|æ¨¡å‹åç§°|æ–‡ä»¶å¤§å°|ä¸‹è½½åœ°å€|å¤‡æ³¨|
|:---:|:---:|:---:|:---:|
|huozi3|88GB|[ğŸ¤—HuggingFace](https://huggingface.co/HIT-SCIR/huozi3)<br>[ModelScope](https://modelscope.cn/models/HIT-SCIR/huozi3/summary)|æ´»å­—3.0 å®Œæ•´æ¨¡å‹|
|huozi3-gguf|25GB|[ğŸ¤—HuggingFace](https://huggingface.co/HIT-SCIR/huozi3-gguf)<br>[ModelScope](https://modelscope.cn/models/HIT-SCIR/huozi3-gguf/summary)|æ´»å­—3.0 GGUFç‰ˆæœ¬ï¼Œé€‚ç”¨äºllama.cppç­‰æ¨ç†æ¡†æ¶|
|huozi3-awq|24GB|[ğŸ¤—HuggingFace](https://huggingface.co/HIT-SCIR/huozi3-awq)<br>[ModelScope](https://modelscope.cn/models/HIT-SCIR/huozi3-awq/summary)|æ´»å­—3.0 AWQç‰ˆæœ¬ï¼Œé€‚ç”¨äºAutoAWQç­‰æ¨ç†æ¡†æ¶|

å¦‚æœæ‚¨å¸Œæœ›å¾®è°ƒæ´»å­—3.0æˆ–Chinese-Mixtral-8x7Bï¼Œè¯·å‚è€ƒ[æ­¤å¤„è®­ç»ƒä»£ç ](https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B?tab=readme-ov-file#%E5%BE%AE%E8%B0%83)ã€‚

## ğŸ’» æ¨¡å‹æ¨ç†

### Quick Start

æ´»å­—3.0é‡‡ç”¨ChatMLæ ¼å¼çš„promptæ¨¡æ¿ï¼Œæ ¼å¼ä¸ºï¼š
```
<|beginofutterance|>ç³»ç»Ÿ
{system prompt}<|endofutterance|>
<|beginofutterance|>ç”¨æˆ·
{input}<|endofutterance|>
<|beginofutterance|>åŠ©æ‰‹
{output}<|endofutterance|>
```

ä½¿ç”¨æ´»å­—3.0è¿›è¡Œæ¨ç†çš„ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
# quickstart.py

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "HIT-SCIR/huozi3"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

text = """<|beginofutterance|>ç³»ç»Ÿ
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹<|endofutterance|>
<|beginofutterance|>ç”¨æˆ·
è¯·ä½ ç”¨pythonå†™ä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç <|endofutterance|>
<|beginofutterance|>åŠ©æ‰‹
"""

inputs = tokenizer(text, return_tensors="pt").to(0)

outputs = model.generate(
    **inputs,
    eos_token_id=57001,
    temperature=0.8,
    top_p=0.9,
    max_new_tokens=2048,
)
print(tokenizer.decode(outputs[0], skip_special_tokens=False))
```

æ´»å­—3.0æ”¯æŒå…¨éƒ¨Mixtralæ¨¡å‹ç”Ÿæ€ï¼ŒåŒ…æ‹¬Transformersã€vLLMã€llama.cppã€AutoAWQã€Text generation web UIç­‰æ¡†æ¶ã€‚

å¦‚æœæ‚¨åœ¨ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨[ModelScope](#modelscope-æ¨¡å‹æ¨ç†)ä¸Šæä¾›çš„æ£€æŸ¥ç‚¹ã€‚

<details>
<summary>

#### Transformers æ¨¡å‹æ¨ç† + æµå¼ç”Ÿæˆ

</summary>

transformersæ”¯æŒä¸ºtokenizeræ·»åŠ èŠå¤©æ¨¡æ¿ï¼Œå¹¶æ”¯æŒæµå¼ç”Ÿæˆã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
# example/transformers-stream/stream.py

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

model_id = "HIT-SCIR/huozi3"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.chat_template = """{% for message in messages %}{{'<|beginofutterance|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|endofutterance|>' + '\n'}}{% endif %}{% endfor %}
{% if add_generation_prompt and messages[-1]['role'] != 'åŠ©æ‰‹' %}{{ '<|beginofutterance|>åŠ©æ‰‹\n' }}{% endif %}"""

chat = [
    {"role": "ç³»ç»Ÿ", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"},
    {"role": "ç”¨æˆ·", "content": "è¯·ä½ ç”¨pythonå†™ä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç "},
]

inputs = tokenizer.apply_chat_template(
    chat,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt",
).to(0)

stream_output = model.generate(
    inputs,
    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),
    eos_token_id=57001,
    temperature=0.8,
    top_p=0.9,
    max_new_tokens=2048,
)
```

</details>

<details>
<summary>

#### ModelScope æ¨¡å‹æ¨ç†

</summary>

ModelScopeçš„æ¥å£ä¸Transformerséå¸¸ç›¸ä¼¼ï¼Œåªéœ€å°†transformersæ›¿æ¢ä¸ºmodelscopeå³å¯ï¼š
```diff
# example/modelscope-generate/generate.py

import torch
- from transformers import AutoModelForCausalLM, AutoTokenizer
+ from modelscope import AutoTokenizer, AutoModelForCausalLM

model_id = "HIT-SCIR/huozi3"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

text = """<|beginofutterance|>ç³»ç»Ÿ
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹<|endofutterance|>
<|beginofutterance|>ç”¨æˆ·
è¯·ä½ ç”¨pythonå†™ä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç <|endofutterance|>
<|beginofutterance|>åŠ©æ‰‹
"""

inputs = tokenizer(text, return_tensors="pt").to(0)

outputs = model.generate(
    **inputs,
    eos_token_id=57001,
    temperature=0.8,
    top_p=0.9,
    max_new_tokens=2048,
)
print(tokenizer.decode(outputs[0], skip_special_tokens=False))
```

</details>

<details>
<summary>

#### vLLM æ¨ç†åŠ é€Ÿ

</summary>

æ´»å­—3.0æ”¯æŒé€šè¿‡vLLMå®ç°æ¨ç†åŠ é€Ÿï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
# example/vllm-generate/generate.py

from vllm import LLM, SamplingParams

prompts = [
    """<|beginofutterance|>ç³»ç»Ÿ
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹<|endofutterance|>
<|beginofutterance|>ç”¨æˆ·
è¯·ä½ ç”¨pythonå†™ä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç <|endofutterance|>
<|beginofutterance|>åŠ©æ‰‹
""",
]

sampling_params = SamplingParams(
    temperature=0.8, top_p=0.95, stop_token_ids=[57001], max_tokens=2048
)
llm = LLM(
    model="HIT-SCIR/huozi3",
    tensor_parallel_size=4,
)
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(generated_text)
```

</details>

<details>
<summary>

#### éƒ¨ç½² OpenAI API Server

</summary>

æ´»å­—3.0å¯ä»¥éƒ¨ç½²ä¸ºæ”¯æŒOpenAI APIåè®®çš„æœåŠ¡ï¼Œè¿™ä½¿å¾—æ´»å­—3.0å¯ä»¥ç›´æ¥é€šè¿‡OpenAI APIè¿›è¡Œè°ƒç”¨ã€‚

ç¯å¢ƒå‡†å¤‡ï¼š
```shell
$ pip install vllm openai
```

å¯åŠ¨æœåŠ¡ï¼š
```shell
$ python -m vllm.entrypoints.openai.api_server --model /path/to/huozi3/checkpoint --served-model-name huozi --chat-template template.jinja --tensor-parallel-size 8 --response-role åŠ©æ‰‹ --max-model-len 2048
```

ä½¿ç”¨OpenAI APIå‘é€è¯·æ±‚ï¼š
```python
# example/openai-api/openai-client.py

from openai import OpenAI

openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model="huozi",
    messages=[
        {"role": "ç³»ç»Ÿ", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"},
        {"role": "ç”¨æˆ·", "content": "è¯·ä½ ç”¨pythonå†™ä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç "},
    ],
    extra_body={"stop_token_ids": [57001]},
)
print("Chat response:", chat_response.choices[0].message.content)
```

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨OpenAI API + Gradio + æµå¼ç”Ÿæˆçš„ç¤ºä¾‹ä»£ç ï¼š
```python
# example/openai-api/openai-client-gradio.py

from openai import OpenAI
import gradio as gr

openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)


def predict(message, history):
    history_openai_format = [
        {"role": "ç³»ç»Ÿ", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"},
    ]
    for human, assistant in history:
        history_openai_format.append({"role": "ç”¨æˆ·", "content": human})
        history_openai_format.append({"role": "åŠ©æ‰‹", "content": assistant})
    history_openai_format.append({"role": "ç”¨æˆ·", "content": message})
    models = client.models.list()

    stream = client.chat.completions.create(
        model=models.data[0].id,
        messages=history_openai_format,
        temperature=0.8,
        stream=True,
        extra_body={"repetition_penalty": 1, "stop_token_ids": [57001]},
    )

    partial_message = ""
    for chunk in stream:
        partial_message += chunk.choices[0].delta.content or ""
        yield partial_message


gr.ChatInterface(predict).queue().launch()
```

</details>

### é‡åŒ–æ¨ç†

æ´»å­—3.0æ”¯æŒé‡åŒ–æ¨ç†ï¼Œä¸‹è¡¨ä¸ºæ´»å­—3.0åœ¨å„ä¸ªé‡åŒ–æ¡†æ¶ä¸‹æ˜¾å­˜å ç”¨é‡ï¼š

|é‡åŒ–æ–¹æ³•|æ˜¾å­˜å ç”¨|
|:---:|:---:|
|æ— |95GB|
|AWQ|32GB|
|GGUF(q4_0)|28GB|
|GGUF(q2_k)|18GB|
|GGUF(q2_k, offload 16å±‚)|9.6GB|

<details>
<summary>

#### GGUF æ ¼å¼

</summary>

GGUFæ ¼å¼æ—¨åœ¨å¿«é€ŸåŠ è½½å’Œä¿å­˜æ¨¡å‹ï¼Œç”±llama.cppå›¢é˜Ÿæ¨å‡ºã€‚æˆ‘ä»¬å·²ç»æä¾›äº†[GGUFæ ¼å¼çš„æ´»å­—3.0](https://huggingface.co/HIT-SCIR/huozi3-gguf)ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å°†HuggingFaceæ ¼å¼çš„æ´»å­—3.0è½¬æ¢åˆ°GGUFæ ¼å¼ï¼Œä»¥ä½¿ç”¨å…¶ä»–çš„é‡åŒ–æ–¹æ³•ã€‚

##### Step 1 ç¯å¢ƒå‡†å¤‡

é¦–å…ˆéœ€è¦ä¸‹è½½llama.cppçš„æºç ã€‚æˆ‘ä»¬åœ¨ä»“åº“ä¸­æä¾›äº†llama.cppçš„submoduleï¼Œè¿™ä¸ªç‰ˆæœ¬çš„llama.cppå·²ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥æˆåŠŸè¿›è¡Œæ¨ç†ï¼š
```shell
$ git clone --recurse-submodules https://github.com/HIT-SCIR/huozi
$ cd examples/llama.cpp
```

æ‚¨ä¹Ÿå¯ä»¥ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„llama.cppæºç ï¼š
```shell
$ git clone https://github.com/ggerganov/llama.cpp.git
$ cd llama.cpp
```

ç„¶åéœ€è¦è¿›è¡Œç¼–è¯‘ã€‚æ ¹æ®æ‚¨çš„ç¡¬ä»¶å¹³å°ï¼Œç¼–è¯‘å‘½ä»¤æœ‰ç»†å¾®å·®å¼‚ï¼š
```shell
$ make  # ç”¨äºçº¯CPUæ¨ç†
$ make LLAMA_CUBLAS=1  # ç”¨äºGPUæ¨ç†
$ LLAMA_METAL=1 make  # ç”¨äºApple Siliconï¼Œæš‚æœªç»è¿‡æµ‹è¯•
```

##### Step 2 æ ¼å¼è½¬æ¢ï¼ˆå¯é€‰ï¼‰

ä»¥ä¸‹å‘½ä»¤éœ€è¦åœ¨`llama.cpp/`ç›®å½•ä¸‹ï¼š
```shell
# è½¬æ¢ä¸ºGGUFæ ¼å¼
$ python convert.py --outfile /path/to/huozi-gguf/huozi3.gguf /path/to/huozi3
# è¿›è¡ŒGGUFæ ¼å¼çš„q4_0é‡åŒ–
$ quantize /path/to/huozi-gguf/huozi3.gguf /path/to/huozi-gguf/huozi3-q4_0.gguf q4_0
```

##### Step 3 å¼€å§‹æ¨ç†

ä»¥ä¸‹å‘½ä»¤éœ€è¦åœ¨`llama.cpp/`ç›®å½•ä¸‹ï¼š
```shell
$ main -m /path/to/huozi-gguf/huozi3-q4_0.gguf --color --interactive-first -c 2048 -t 6 --temp 0.2 --repeat_penalty 1.1 -ngl 999 --in-prefix "<|beginofutterance|>ç”¨æˆ·\n" --in-suffix "<|endofutterance|>\n<|beginofutterance|>åŠ©æ‰‹" -r "<|endofutterance|>"
```

`-ngl`å‚æ•°è¡¨ç¤ºå‘GPUä¸­offloadçš„å±‚æ•°ï¼Œé™ä½è¿™ä¸ªå€¼å¯ä»¥ç¼“è§£GPUæ˜¾å­˜å‹åŠ›ã€‚ç»è¿‡æˆ‘ä»¬çš„å®é™…æµ‹è¯•ï¼Œq2_ké‡åŒ–çš„æ¨¡å‹offload 16å±‚ï¼Œæ˜¾å­˜å ç”¨å¯é™ä½è‡³9.6GBï¼Œå¯åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œæ¨¡å‹ï¼š
```shell
$ main -m /path/to/huozi-gguf/huozi3-q2_k.gguf --color --interactive-first -c 2048 -t 6 --temp 0.2 --repeat_penalty 1.1 -ngl 16 --in-prefix "<|beginofutterance|>ç”¨æˆ·\n" --in-suffix "<|endofutterance|>\n<|beginofutterance|>åŠ©æ‰‹" -r "<|endofutterance|>"
```

å…³äº`main`çš„æ›´å¤šå‚æ•°ï¼Œå¯ä»¥å‚è€ƒllama.cppçš„[å®˜æ–¹æ–‡æ¡£](https://github.com/ggerganov/llama.cpp/tree/master/examples/main)ã€‚

</details>

<details>
<summary>

#### AWQ æ ¼å¼

</summary>

AWQæ˜¯ä¸€ç§é‡åŒ–æ¨¡å‹çš„å­˜å‚¨æ ¼å¼ã€‚æˆ‘ä»¬å·²ç»æä¾›äº†[AWQæ ¼å¼çš„æ´»å­—3.0](https://huggingface.co/HIT-SCIR/huozi3-awq)ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å°†HuggingFaceæ ¼å¼çš„æ´»å­—3.0è½¬æ¢åˆ°AWQæ ¼å¼ã€‚

##### Step 1 æ ¼å¼è½¬æ¢ï¼ˆå¯é€‰ï¼‰

```python
# example/autoawq-generate/quant.py

from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = "/path/to/huozi3"
quant_path = "/path/to/save/huozi3-awq"
modules_to_not_convert = ["gate"]
quant_config = {
    "zero_point": True,
    "q_group_size": 128,
    "w_bit": 4,
    "version": "GEMM",
    "modules_to_not_convert": modules_to_not_convert,
}

model = AutoAWQForCausalLM.from_pretrained(
    model_path,
    safetensors=True,
    **{"low_cpu_mem_usage": True},
)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model.quantize(
    tokenizer,
    quant_config=quant_config,
    modules_to_not_convert=modules_to_not_convert,
)

model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)

print(f'Model is quantized and saved at "{quant_path}"')
```

##### Step 2 å¼€å§‹æ¨ç†

åœ¨è·å–åˆ°AWQæ ¼å¼çš„æ¨¡å‹æƒé‡åï¼Œå¯ä»¥ä½¿ç”¨AutoAWQForCausalLMä»£æ›¿AutoModelForCausalLMåŠ è½½æ¨¡å‹ã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```diff
# example/autoawq-generate/generate.py

import torch
+ from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer, TextStreamer


- model_id = "HIT-SCIR/huozi3"
+ model_id = "HIT-SCIR/huozi3-awq"  # or model_id = "/path/to/saved/huozi3-awq"

+ model = AutoAWQForCausalLM.from_quantized(model_id, fuse_layers=True)
- model = AutoModelForCausalLM.from_pretrained(
-     model_id,
-     attn_implementation="flash_attention_2",
-     torch_dtype=torch.bfloat16,
-     device_map="auto",
- )

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.chat_template = """{% for message in messages %}{{'<|beginofutterance|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|endofutterance|>' + '\n'}}{% endif %}{% endfor %}
{% if add_generation_prompt and messages[-1]['role'] != 'åŠ©æ‰‹' %}{{ '<|beginofutterance|>åŠ©æ‰‹\n' }}{% endif %}"""

chat = [
    {"role": "ç³»ç»Ÿ", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"},
    {"role": "ç”¨æˆ·", "content": "è¯·ä½ ç”¨pythonå†™ä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç "},
]

inputs = tokenizer.apply_chat_template(
    chat,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt",
).to(0)

stream_output = model.generate(
    inputs,
    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),
    eos_token_id=57001,
    temperature=0.8,
    top_p=0.9,
    max_new_tokens=2048,
)
```

</details>

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

![](image/metric-v3-h.png)

é’ˆå¯¹å¤§æ¨¡å‹ç»¼åˆèƒ½åŠ›è¯„ä»·ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ä»¥ä¸‹è¯„æµ‹æ•°æ®é›†å¯¹æ´»å­—3.0è¿›è¡Œè¯„æµ‹ï¼š
- C-Evalï¼šä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ã€‚å®ƒåŒ…å«äº†13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–äº†52ä¸ªä¸åŒçš„å­¦ç§‘å’Œå››ä¸ªéš¾åº¦çº§åˆ«ã€‚
- CMMLUï¼šä¸€ä¸ªç»¼åˆæ€§çš„ä¸­æ–‡è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–äº†ä»åŸºç¡€å­¦ç§‘åˆ°é«˜çº§ä¸“ä¸šæ°´å¹³çš„67ä¸ªä¸»é¢˜ã€‚
- GAOKAOï¼šä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ç›®ä¸ºæ•°æ®é›†ï¼Œæ—¨åœ¨æä¾›å’Œäººç±»å¯¹é½çš„ï¼Œç›´è§‚ï¼Œé«˜æ•ˆåœ°æµ‹è¯„å¤§æ¨¡å‹è¯­è¨€ç†è§£èƒ½åŠ›ã€é€»è¾‘æ¨ç†èƒ½åŠ›çš„æµ‹è¯„æ¡†æ¶ã€‚
- MMLUï¼šä¸€ä¸ªåŒ…å«57ä¸ªå¤šé€‰ä»»åŠ¡çš„è‹±æ–‡è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†åˆç­‰æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦ã€æ³•å¾‹ç­‰ï¼Œéš¾åº¦è¦†ç›–é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ï¼Œæ˜¯ç›®å‰ä¸»æµçš„LLMè¯„æµ‹æ•°æ®é›†ä¹‹ä¸€ã€‚
- HellaSwagï¼šä¸€ä¸ªæå…·æŒ‘æˆ˜çš„è‹±æ–‡NLIè¯„æµ‹æ•°æ®é›†ï¼Œæ¯ä¸€ä¸ªé—®é¢˜éƒ½éœ€è¦å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œæ·±å…¥ç†è§£ï¼Œè€Œä¸èƒ½åŸºäºå¸¸è¯†è¿›è¡Œå›ç­”ã€‚
- GSM8Kï¼šä¸€ä¸ªé«˜è´¨é‡çš„å°å­¦æ•°å­¦åº”ç”¨é¢˜çš„æ•°æ®é›†ï¼Œè¿™äº›é—®é¢˜éœ€è¦ 2 åˆ° 8 ä¸ªæ­¥éª¤æ¥è§£å†³ï¼Œè§£å†³æ–¹æ¡ˆä¸»è¦æ¶‰åŠä½¿ç”¨åŸºæœ¬ç®—æœ¯è¿ç®—ï¼Œå¯ç”¨äºè¯„ä»·å¤šæ­¥æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚
- HumanEvalï¼šä¸€ä¸ªç”± 164 ä¸ªåŸåˆ›ç¼–ç¨‹é—®é¢˜ç»„æˆçš„æ•°æ®é›†ï¼Œé€šè¿‡è¡¡é‡ä»æ–‡æ¡£å­—ç¬¦ä¸²ç”Ÿæˆç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œæ¥å¤Ÿè¯„ä¼°è¯­è¨€ç†è§£ã€ç®—æ³•å’Œç®€å•çš„æ•°å­¦èƒ½åŠ›ã€‚
- MT-Benchï¼šä¸€ä¸ªå¼€æ”¾çš„è‹±æ–‡é—®é¢˜é›†ï¼ŒåŒ…æ‹¬80ä¸ªå¤šè½®å¯¹è¯ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°èŠå¤©æœºå™¨äººçš„å¤šè½®å¯¹è¯å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤§æ¨¡å‹è£åˆ¤ï¼ˆGPT-4ï¼‰å¯¹æ¨¡å‹å›ç­”è¿›è¡Œæ‰“åˆ†ã€‚
- MT-Bench-zhï¼šæˆ‘ä»¬æ ¹æ®MT-Benchç¿»è¯‘å¾—æ¥çš„ä¸­æ–‡é—®é¢˜é›†ï¼Œæ¯ç»„é—®é¢˜å‡ç»è¿‡äººå·¥æ ¡å¯¹å’Œä¸­æ–‡è¯­å¢ƒä¸‹çš„é€‚å½“è°ƒæ•´ã€‚æˆ‘ä»¬å·²åœ¨[æ­¤å¤„](data/mt-bench-zh/)å¼€æºMT-Bench-zhæ•°æ®é›†ã€‚
- MT-Bench-safetyï¼šæˆ‘ä»¬æ‰‹å·¥æ„é€ çš„å®‰å…¨æ•°æ®é›†ï¼ŒåŒ…æ‹¬æš´åŠ›ã€è‰²æƒ…ã€æ•æ„Ÿç­‰é£é™©å†…å®¹ã€‚è¯¥æ•°æ®é›†ä¸ºå°é—­æ•°æ®é›†ã€‚

æ´»å­—3.0åœ¨æ¨ç†æ—¶ä»…æ¿€æ´»13Bå‚æ•°ã€‚ä¸‹è¡¨ä¸ºæ´»å­—3.0ä¸å…¶ä»–13Bè§„æ¨¡çš„ä¸­æ–‡æ¨¡å‹ä»¥åŠæ—§ç‰ˆæ´»å­—åœ¨å„ä¸ªè¯„æµ‹æ•°æ®é›†ä¸Šçš„ç»“æœï¼š

<!-- | æ¨¡å‹åç§°                                      | æ¨¡å‹ç»“æ„ | C-Eval<br>(ä¸­æ–‡) | CMMLU<br>(ä¸­æ–‡) | GAOKAO<br>(ä¸­æ–‡) | MT-Bench-zh<br>(ä¸­æ–‡å¯¹è¯) | MT-Bench-safety<br>(ä¸­æ–‡å®‰å…¨) | MMLU<br>(è‹±æ–‡) | HellaSwag<br>(è‹±æ–‡) | MT-Bench<br>(è‹±æ–‡å¯¹è¯) | GSM8K<br>(æ•°å­¦) | HumanEval<br>(ä»£ç ) |
|---------------------------------------------|---------|--------------|-------------|---------------|--------------------------|-----------------------------|------------|------------------|-----------------------|-------------|-----------------|
| baichuan-inc/Baichuan2-13B-Chat v2         | Baichuan| 56.13        | 58.50       | 48.99         | 6.74                     | 8.30                        | 54.50      | 51.19            | 6.59                  | 25.17       | 20.12           |
| wangrongsheng/Aurora-Plus                   | Mixtral | 47.67        | 48.75       | 35.05         | 5.47                     | 6.70                        | 67.80      | 78.27            | 7.13                  | 66.26       | 27.44           |
| TigerResearch/tigerbot-13b-chat-v5         | LLaMA   | 49.78        | 51.28       | 41.31         | 5.98                     | 7.63                        | 56.34      | 35.17            | 4.88                  | 66.19       | 14.63           |
| hfl/chinese-alpaca-2-13b                   | LLaMA   | 43.47        | 44.53       | 25.94         | 5.77                     | 8.13                        | 53.05      | 56.85            | 6.24                  | 32.75       | 14.02           |
| æ´»å­—1.0                                      | BLOOM   | 37.27        | 36.24       | 19.72         | 4.48                     | 7.18                        | 39.68      | 33.21            | 4.34                  | 21.99       | 13.41           |
| æ´»å­—2.0                                      | BLOOM   | 32.05        | 34.68       | 22.97         | 5.08                     | 6.68                        | 38.04      | 33.34            | 4.79                  | 19.86       | 6.71            |
| **æ´»å­—3.0ï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰**                          | Mixtral | 51.82        | 51.06       | 41.21         | 6.29                     | 7.58                        | 69.48      | 65.18            | 7.62                  | 65.81       | 40.85           | -->
![](image/evaluation-v3.png)

> æˆ‘ä»¬åœ¨C-Evalã€CMMLUã€MMLUé‡‡ç”¨5-shotï¼ŒGSM8Ké‡‡ç”¨4-shotï¼ŒHellaSwagã€HumanEvalé‡‡ç”¨0-shotï¼ŒHumanEvalé‡‡ç”¨pass@1æŒ‡æ ‡ã€‚æ‰€æœ‰æµ‹è¯•å‡é‡‡ç”¨greedyç­–ç•¥ã€‚
>
> æˆ‘ä»¬ä½¿ç”¨OpenCompassä½œä¸ºè¯„æµ‹æ¡†æ¶ï¼Œcommit hashä¸º[4c87e77](https://github.com/open-compass/opencompass/tree/4c87e777d855636b9eda7ec87bcbbf12b62caed3)ã€‚è¯„æµ‹ä»£ç ä½äº[æ­¤å¤„](./evaluate/)ã€‚

æ ¹æ®ä¸Šè¡¨ä¸­çš„æµ‹è¯•ç»“æœï¼Œæ´»å­—3.0è¾ƒæ—§ç‰ˆæ´»å­—å–å¾—äº†å·¨å¤§çš„æ€§èƒ½æå‡ã€‚åœ¨ä¸­æ–‡çŸ¥è¯†æ–¹é¢ï¼Œæ´»å­—3.0è¾¾åˆ°äº†ä¸Tigerbot-13B-chat-v5ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶æ˜¯åœ¨ä¸­æ–‡å¯¹è¯å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°å¾—æ›´åŠ ä¼˜ç§€ã€‚åœ¨è‹±æ–‡çŸ¥è¯†æ–¹é¢ï¼Œå¾—ç›ŠäºåŸç‰ˆMixtral-8x7Bçš„å¼ºå¤§æ€§èƒ½ï¼Œæ´»å­—3.0è¶…è¿‡äº†Baichuan2-13B-Chat v2å’ŒLLaMAç³»åˆ—çš„æ‰©è¯è¡¨æ¨¡å‹ï¼Œå¹¶åœ¨è‹±æ–‡å¯¹è¯å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šè¾¾åˆ°äº†è¾ƒé«˜æ°´å¹³ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæ´»å­—3.0å‡å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¿™è¯´æ˜æ´»å­—3.0å¯¹å¤æ‚é—®é¢˜çš„æ·±å±‚æ¬¡ç†è§£ã€å¤šæ­¥æ¨ç†ã€ä»¥åŠç»“æ„åŒ–ä¿¡æ¯å¤„ç†ç­‰æ–¹é¢å…·æœ‰è¾ƒå¼ºæ°´å¹³ã€‚ç”±äºæˆ‘ä»¬é‡‡ç”¨äº†è¾ƒé«˜è´¨é‡çš„ä»£ç æ•°æ®é›†ï¼Œæ´»å­—3.0çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ä¹Ÿè¶…è¶Šäº†åŒä¸ºMixtralç»“æ„çš„Aurora-Plusæ¨¡å‹ã€‚

## ğŸ—‚ ç”Ÿæˆæ ·ä¾‹

ä¸‹é¢æ˜¯æ´»å­—3.0åœ¨MT-Bench-zhè¯„æµ‹é›†ä¸Šçš„ç”Ÿæˆæ•ˆæœå±•ç¤ºï¼Œå¹¶ä¸æ´»å­—2.0ï¼ˆRLHFç‰ˆæœ¬ï¼‰è¿›è¡Œå¯¹æ¯”ï¼š

![](image/examples/v3-case1.png)
![](image/examples/v3-case2.png)
![](image/examples/v3-case3.png)
![](image/examples/v3-case4.png)
![](image/examples/v3-case5.png)

## <img src="https://cdn.jsdelivr.net/gh/LightChen233/blog-img/folders.png" width="25" /> å¼€æºåè®®
å¯¹æœ¬ä»“åº“æºç çš„ä½¿ç”¨éµå¾ªå¼€æºè®¸å¯åè®® [Apache 2.0](https://github.com/HIT-SCIR/huozi/blob/main/LICENSE)ã€‚

æ´»å­—æ”¯æŒå•†ç”¨ã€‚å¦‚æœå°†æ´»å­—æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨ä½œå•†ä¸šç”¨é€”ï¼Œè¯·æ‚¨æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è”ç³»è®¸å¯æ–¹ï¼Œä»¥è¿›è¡Œç™»è®°å¹¶å‘è®¸å¯æ–¹ç”³è¯·ä¹¦é¢æˆæƒï¼šè”ç³»é‚®ç®±ï¼š<jngao@ir.hit.edu.cn>ã€‚

## <img src="https://cdn.jsdelivr.net/gh/LightChen233/blog-img/notes.png" width="25" /> Citation

### æ´»å­—å¤§æ¨¡å‹

```latex
@misc{huozi,
    author = {Huozi-Team}.
    title = {Huozi: Leveraging Large Language Models for Enhanced Open-Domain Chatting}
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository}
    howpublished = {\url{https://github.com/HIT-SCIR/huozi}}
}
```

## <img src="https://cdn.jsdelivr.net/gh/LightChen233/blog-img/star.png" width="25" /> Star History

[![Star History Chart](https://api.star-history.com/svg?repos=HIT-SCIR/huozi&type=Date)](https://star-history.com/#HIT-SCIR/huozi&Date)
